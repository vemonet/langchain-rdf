{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦙 Local RAG for OWL ontologies\n",
    "\n",
    "Demo of **Retrieval Augmented Generation** (RAG) to faithfully resolve and use concepts from an OWL ontology, with conversation memory, running locally, using only open source components:\n",
    "* [LangChain](https://python.langchain.com) (cf. docs: [RAG with memory](https://python.langchain.com/docs/expression_language/cookbook/retrieval), [streaming RAG](https://python.langchain.com/docs/use_cases/question_answering/streaming))\n",
    "* [FastEmbed embeddings](https://github.com/qdrant/fastembed)\n",
    "* [Qdrant vectorstore](https://github.com/qdrant/qdrant)\n",
    "* [LlamaCpp inference library](https://github.com/ggerganov/llama.cpp)\n",
    "* [Mixtral 8x7B LLM](https://mistral.ai/news/mixtral-of-experts/)\n",
    "\n",
    "This demo runs locally on CPU and GPU, but will be considerably slow on CPU (a few minutes to answer the question).\n",
    "\n",
    "You can easily change the different components used in this workflow to use whatever you prefer thanks to LangChain: \n",
    "* LLM (e.g. switch to [ChatGPT](https://python.langchain.com/docs/integrations/llms/openai), Claude)\n",
    "* Vectorstore (e.g. switch to [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss), [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma), Milvus)\n",
    "* Embedding model (e.g. switch to [HuggingFace sentence transformer](https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers), OpenAI ADA)\n",
    "\n",
    "## 📦️ Install and import dependencies\n",
    "\n",
    "First download the Mixtral 8x7B model in GGUF format (~15G) in the `notebooks/data/` folder:\n",
    "\n",
    "```bash\n",
    "wget https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/resolve/main/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\n",
    "```\n",
    "\n",
    "> Make sure to pick up a model already fine-tuned for chat (they have `instruct` or `chat` in their name usually)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.6)\n",
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.0.19)\n",
      "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.39)\n",
      "Requirement already satisfied: fastembed in /usr/local/lib/python3.10/dist-packages (0.2.1)\n",
      "Requirement already satisfied: qdrant-client in /usr/local/lib/python3.10/dist-packages (1.7.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.26)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.22)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.20.3)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.7.2)\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (1.15.0)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from fastembed) (1.17.0)\n",
      "Requirement already satisfied: tokenizers<0.16.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from fastembed) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /usr/local/lib/python3.10/dist-packages (from fastembed) (4.66.2)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.60.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (1.60.1)\n",
      "Requirement already satisfied: httpx>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (0.26.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.10/dist-packages (from qdrant-client) (2.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /usr/local/lib/python3.10/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (4.25.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from grpcio-tools>=1.41.0->qdrant-client) (59.6.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.0.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (3.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]>=0.14.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.21,>=0.20->fastembed) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.21,>=0.20->fastembed) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.21,>=0.20->fastembed) (23.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (23.5.26)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed) (1.12)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant-client) (1.2.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant-client) (4.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install langchain langchain-community llama-cpp-python fastembed qdrant-client\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Any\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_rdf import OntologyLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 Initialize local vectorstore and LLM\n",
    "\n",
    "```\n",
    "flag_embeddings_size = 384\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-12 13:10:29.251\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated. Use TextEmbedding instead.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736a79540e7d4e7684453d8b194d93b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 995 tensors from ./data/mixtral-8x7b-instruct-v0.1.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:                         llama.expert_count u32              = 8\n",
      "llama_model_loader: - kv  10:                    llama.expert_used_count u32              = 2\n",
      "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  12:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:   32 tensors\n",
      "llama_model_loader: - type q8_0:   64 tensors\n",
      "llama_model_loader: - type q2_K:  801 tensors\n",
      "llama_model_loader: - type q3_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 8\n",
      "llm_load_print_meta: n_expert_used    = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 46.70 B\n",
      "llm_load_print_meta: model size       = 14.57 GiB (2.68 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mixtral-8x7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.38 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 14918.57 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     3.09 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'llama.expert_count': '8', 'llama.context_length': '32768', 'general.name': 'mistralai_mixtral-8x7b-instruct-v0.1', 'llama.expert_used_count': '2'}\n",
      "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
      "Using chat eos_token: \n",
      "Using chat bos_token: \n"
     ]
    }
   ],
   "source": [
    "flag_embeddings = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", max_length=512)\n",
    "loader = OntologyLoader(\"https://semanticscience.org/ontology/sio.owl\", format=\"xml\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into chunks if necessary\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    splits,\n",
    "    flag_embeddings,\n",
    "    collection_name=\"ontologies\",\n",
    "    location=\":memory:\",\n",
    "    # path=\"./data/qdrant\",\n",
    "    # Run Qdrant as a service for production use:\n",
    "    # url=\"http://localhost:6333\",\n",
    "    # prefer_grpc=True,\n",
    ")\n",
    "# vectorstore = FAISS.from_documents(documents=docs, embedding=flag_embeddings)\n",
    "# K is the number of source documents retrieved\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./data/mixtral-8x7b-instruct-v0.1.Q2_K.gguf\",\n",
    "    temperature=0.01,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    n_threads=8,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    # n_gpu_layers=40,  # Change this value based on your model and your GPU VRAM pool.\n",
    "    # n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Initialize prompts and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the memory object that is used to add messages\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "# Add a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "# Prompt to reformulate the question using the chat history\n",
    "reform_template = \"\"\"Given the following chat history and a follow up question,\n",
    "rephrase the follow up question to be a standalone straightforward question, in its original language.\n",
    "Do not answer the question! Just rephrase reusing informations from the chat history.\n",
    "Make it short and straight to the point.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "REFORM_QUESTION_PROMPT = PromptTemplate.from_template(reform_template)\n",
    "\n",
    "# Prompt to ask to answer the reformulated question\n",
    "answer_template = \"\"\"Briefly answer the question based only on the following context,\n",
    "do not use any information outside this context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "# Format how the ontology concepts are passed as context to the LLM\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    template=\"Concept label: {page_content} | URI: {uri} | Type: {type} | Predicate: {predicate} | Ontology: {ontology}\"\n",
    ")\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    # print(\"Formatted docs:\", doc_strings)\n",
    "    return document_separator.join(doc_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⛓️ Define the chain\n",
    "\n",
    "`itemgetter()` is used to retrieve objects defined in the previous step in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformulate the question using chat history\n",
    "reformulated_question = {\n",
    "    \"reformulated_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | REFORM_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Retrieve the documents using the reformulated question\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"reformulated_question\") | retriever,\n",
    "    \"question\": lambda x: print(\"💭 Reformulated question:\", x[\"reformulated_question\"]) or x[\"reformulated_question\"],\n",
    "    # \"question\": lambda x: x[\"reformulated_question\"],\n",
    "}\n",
    "# Construct the inputs for the final prompt using retrieved documents\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# Generate the answer using the retrieved documents and answer prompt\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# Put the chain together\n",
    "final_chain = loaded_memory | reformulated_question | retrieved_documents | answer\n",
    "\n",
    "def stream_chain(final_chain, memory: ConversationBufferMemory, inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    \"\"\"Ask question, stream the answer output, and return the answer with source documents.\"\"\"\n",
    "    output = {\"answer\": \"\"}\n",
    "    for chunk in final_chain.stream(inputs):\n",
    "        if \"docs\" in chunk:\n",
    "            output[\"docs\"] = [doc.dict() for doc in chunk[\"docs\"]]\n",
    "            print(\"📚 Documents retrieved:\")\n",
    "            for doc in output[\"docs\"]:\n",
    "                print(f\"· {doc['page_content']} ({doc['metadata']['uri']})\")\n",
    "            # print(json.dumps(output[\"docs\"], indent=2))\n",
    "        if \"answer\" in chunk:\n",
    "            output[\"answer\"] += chunk[\"answer\"]\n",
    "            print(chunk[\"answer\"], end=\"\", flush=True)\n",
    "    # Add messages to chat history\n",
    "    memory.save_context(inputs, {\"answer\": output[\"answer\"]})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗨️ Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     386.65 ms\n",
      "llama_print_timings:      sample time =       2.06 ms /     6 runs   (    0.34 ms per token,  2912.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4039.45 ms /    80 tokens (   50.49 ms per token,    19.80 tokens per second)\n",
      "llama_print_timings:        eval time =     603.95 ms /     6 runs   (  100.66 ms per token,     9.93 tokens per second)\n",
      "llama_print_timings:       total time =    4696.17 ms /    86 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💭 Reformulated question:  What is a protein?\n",
      "📚 Documents retrieved:\n",
      "· protein (http://semanticscience.org/resource/SIO_010043)\n",
      "· A protein is an organic polymer that is composed of one or more linear polymers of amino acids. (http://semanticscience.org/resource/SIO_010043)\n",
      "· A protein complex is a molecular complex composed of at least two polypeptide chains. (http://semanticscience.org/resource/SIO_010497)\n",
      "· protein family (http://semanticscience.org/resource/SIO_001380)\n",
      "· amino acid (http://semanticscience.org/resource/SIO_001224)\n",
      "\n",
      "Answer: A protein is an organic polymer that is composed of one or more linear polymers of amino acids."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     386.65 ms\n",
      "llama_print_timings:      sample time =      11.04 ms /    29 runs   (    0.38 ms per token,  2625.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   33983.18 ms /   558 tokens (   60.90 ms per token,    16.42 tokens per second)\n",
      "llama_print_timings:        eval time =    4154.86 ms /    28 runs   (  148.39 ms per token,     6.74 tokens per second)\n",
      "llama_print_timings:       total time =   38453.58 ms /   586 tokens\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "answer": "\nAnswer: A protein is an organic polymer that is composed of one or more linear polymers of amino acids.",
       "docs": [
        {
         "metadata": {
          "_collection_name": "ontologies",
          "_id": "e2c56541326543dc8de4c374fc8ee2be",
          "label": "protein",
          "ontology": "https://semanticscience.org/ontology/sio.owl",
          "predicate": "http://www.w3.org/2000/01/rdf-schema#label",
          "type": "http://www.w3.org/2002/07/owl#Class",
          "uri": "http://semanticscience.org/resource/SIO_010043"
         },
         "page_content": "protein",
         "type": "Document"
        },
        {
         "metadata": {
          "_collection_name": "ontologies",
          "_id": "ce5da14462ea4f63821a3e124c8ded1d",
          "label": "A protein is an organic polymer that is composed of one or more linear polymers of amino acids.",
          "ontology": "https://semanticscience.org/ontology/sio.owl",
          "predicate": "http://purl.org/dc/terms/description",
          "type": "http://www.w3.org/2002/07/owl#Class",
          "uri": "http://semanticscience.org/resource/SIO_010043"
         },
         "page_content": "A protein is an organic polymer that is composed of one or more linear polymers of amino acids.",
         "type": "Document"
        },
        {
         "metadata": {
          "_collection_name": "ontologies",
          "_id": "fa2c6d021d9e45d3b276d99eacf1afec",
          "label": "A protein complex is a molecular complex composed of at least two polypeptide chains.",
          "ontology": "https://semanticscience.org/ontology/sio.owl",
          "predicate": "http://purl.org/dc/terms/description",
          "type": "http://www.w3.org/2002/07/owl#Class",
          "uri": "http://semanticscience.org/resource/SIO_010497"
         },
         "page_content": "A protein complex is a molecular complex composed of at least two polypeptide chains.",
         "type": "Document"
        },
        {
         "metadata": {
          "_collection_name": "ontologies",
          "_id": "579047aff007414bbe5d41d6357360f3",
          "label": "protein family",
          "ontology": "https://semanticscience.org/ontology/sio.owl",
          "predicate": "http://www.w3.org/2000/01/rdf-schema#label",
          "type": "http://www.w3.org/2002/07/owl#Class",
          "uri": "http://semanticscience.org/resource/SIO_001380"
         },
         "page_content": "protein family",
         "type": "Document"
        },
        {
         "metadata": {
          "_collection_name": "ontologies",
          "_id": "8612a2115a7d45e399e53b90d3c700be",
          "label": "amino acid",
          "ontology": "https://semanticscience.org/ontology/sio.owl",
          "predicate": "http://www.w3.org/2000/01/rdf-schema#label",
          "type": "http://www.w3.org/2002/07/owl#Class",
          "uri": "http://semanticscience.org/resource/SIO_001224"
         },
         "page_content": "amino acid",
         "type": "Document"
        }
       ]
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set_debug(True)   # Uncomment to enable detailed LangChain debugging\n",
    "output = stream_chain(final_chain, memory, {\n",
    "    \"question\": \"What is a protein?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     386.65 ms\n",
      "llama_print_timings:      sample time =       4.07 ms /    12 runs   (    0.34 ms per token,  2948.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8436.46 ms /   124 tokens (   68.04 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1616.31 ms /    11 runs   (  146.94 ms per token,     6.81 tokens per second)\n",
      "llama_print_timings:       total time =   10136.51 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💭 Reformulated question:  What is the URI for \"protein\" concept?\n",
      "📚 Documents retrieved:\n",
      "· protein (http://semanticscience.org/resource/SIO_010043)\n",
      "· protein complex (http://semanticscience.org/resource/SIO_010497)\n",
      "· protein-protein association (http://semanticscience.org/resource/SIO_001438)\n",
      "· A protein complex is a molecular complex composed of at least two polypeptide chains. (http://semanticscience.org/resource/SIO_010497)\n",
      "· A protein-protein association is an association between two proteins. (http://semanticscience.org/resource/SIO_001438)\n",
      "Answer:  The URI for \"protein\" concept is <http://semanticscience.org/resource/SIO_010043>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     386.65 ms\n",
      "llama_print_timings:      sample time =      14.52 ms /    35 runs   (    0.41 ms per token,  2410.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =   31457.99 ms /   566 tokens (   55.58 ms per token,    17.99 tokens per second)\n",
      "llama_print_timings:        eval time =    3581.62 ms /    34 runs   (  105.34 ms per token,     9.49 tokens per second)\n",
      "llama_print_timings:       total time =   35417.71 ms /   600 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'Answer:  The URI for \"protein\" concept is <http://semanticscience.org/resource/SIO_010043>',\n",
       " 'docs': [{'page_content': 'protein',\n",
       "   'metadata': {'label': 'protein',\n",
       "    'uri': 'http://semanticscience.org/resource/SIO_010043',\n",
       "    'type': 'http://www.w3.org/2002/07/owl#Class',\n",
       "    'predicate': 'http://www.w3.org/2000/01/rdf-schema#label',\n",
       "    'ontology': 'https://semanticscience.org/ontology/sio.owl',\n",
       "    '_id': 'e2c56541326543dc8de4c374fc8ee2be',\n",
       "    '_collection_name': 'ontologies'},\n",
       "   'type': 'Document'},\n",
       "  {'page_content': 'protein complex',\n",
       "   'metadata': {'label': 'protein complex',\n",
       "    'uri': 'http://semanticscience.org/resource/SIO_010497',\n",
       "    'type': 'http://www.w3.org/2002/07/owl#Class',\n",
       "    'predicate': 'http://www.w3.org/2000/01/rdf-schema#label',\n",
       "    'ontology': 'https://semanticscience.org/ontology/sio.owl',\n",
       "    '_id': '23dec66d2746454892b1829632024757',\n",
       "    '_collection_name': 'ontologies'},\n",
       "   'type': 'Document'},\n",
       "  {'page_content': 'protein-protein association',\n",
       "   'metadata': {'label': 'protein-protein association',\n",
       "    'uri': 'http://semanticscience.org/resource/SIO_001438',\n",
       "    'type': 'http://www.w3.org/2002/07/owl#Class',\n",
       "    'predicate': 'http://www.w3.org/2000/01/rdf-schema#label',\n",
       "    'ontology': 'https://semanticscience.org/ontology/sio.owl',\n",
       "    '_id': 'c3cb5cdb20454555ab79910500843783',\n",
       "    '_collection_name': 'ontologies'},\n",
       "   'type': 'Document'},\n",
       "  {'page_content': 'A protein complex is a molecular complex composed of at least two polypeptide chains.',\n",
       "   'metadata': {'label': 'A protein complex is a molecular complex composed of at least two polypeptide chains.',\n",
       "    'uri': 'http://semanticscience.org/resource/SIO_010497',\n",
       "    'type': 'http://www.w3.org/2002/07/owl#Class',\n",
       "    'predicate': 'http://purl.org/dc/terms/description',\n",
       "    'ontology': 'https://semanticscience.org/ontology/sio.owl',\n",
       "    '_id': 'fa2c6d021d9e45d3b276d99eacf1afec',\n",
       "    '_collection_name': 'ontologies'},\n",
       "   'type': 'Document'},\n",
       "  {'page_content': 'A protein-protein association is an association between two proteins.',\n",
       "   'metadata': {'label': 'A protein-protein association is an association between two proteins.',\n",
       "    'uri': 'http://semanticscience.org/resource/SIO_001438',\n",
       "    'type': 'http://www.w3.org/2002/07/owl#Class',\n",
       "    'predicate': 'http://www.w3.org/2000/01/rdf-schema#comment',\n",
       "    'ontology': 'https://semanticscience.org/ontology/sio.owl',\n",
       "    '_id': '19693633c2804cdebf27e51b5ef6a739',\n",
       "    '_collection_name': 'ontologies'},\n",
       "   'type': 'Document'}]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_chain(final_chain, memory, {\n",
    "    \"question\": \"What is the URI for this concept?\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libre-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
