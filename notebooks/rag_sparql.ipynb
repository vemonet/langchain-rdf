{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🦙 RAG for SPARQL queries\n",
    "\n",
    "Demo of **Retrieval Augmented Generation** (RAG) to help writing SPARQL queries using examples published to a SAPRQL endpoint using SHACL ontology, with conversation memory, running locally, using only open source components:\n",
    "* [LangChain](https://python.langchain.com) (cf. docs: [RAG with memory](https://python.langchain.com/docs/expression_language/cookbook/retrieval), [streaming RAG](https://python.langchain.com/docs/use_cases/question_answering/streaming))\n",
    "* [FastEmbed embeddings](https://github.com/qdrant/fastembed)\n",
    "* [Qdrant vectorstore](https://github.com/qdrant/qdrant)\n",
    "* [Ollama inference library](https://www.ollama.com)\n",
    "* [Llama3 8B LLM](https://llama.meta.com/llama3/) or [Mixtral LLM](https://mistral.ai/news/mixtral-of-experts/)\n",
    "\n",
    "This demo runs locally on CPU and GPU, but will be considerably slow on CPU (a few minutes to answer the question).\n",
    "\n",
    "You can easily change the different components used in this workflow to use whatever you prefer thanks to LangChain: \n",
    "* LLM (e.g. switch to [ChatGPT](https://python.langchain.com/docs/integrations/llms/openai), Claude)\n",
    "* Vectorstore (e.g. switch to [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss), [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma), Milvus)\n",
    "* Embedding model (e.g. switch to [HuggingFace sentence transformer](https://python.langchain.com/docs/integrations/text_embedding/sentence_transformers), OpenAI ADA)\n",
    "\n",
    "## 📦️ Install and import dependencies\n",
    "\n",
    "⚠️ Install `ollama`: https://ollama.com/download\n",
    "\n",
    "And pull the model you will use:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3\n",
    "```\n",
    "\n",
    "> You can also try with `mixtral:8x22b`, just think to change the call to `ChatOllama` in the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --quiet langchain langchain-community llama-cpp-python langchain-qdrant fastembed langchain-openai\n",
    "\n",
    "from operator import itemgetter\n",
    "from typing import Any\n",
    "\n",
    "from langchain.globals import set_debug\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_rdf import SparqlExamplesLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌀 Initialize local vectorstore and LLM\n",
    "\n",
    "```\n",
    "flag_embeddings_size = 384\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13172963acb4392941f6c94c1313f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\", max_length=512)\n",
    "loader = SparqlExamplesLoader(\"https://sparql.uniprot.org/sparql/\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the documents into chunks if necessary\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embedding_model,\n",
    "    collection_name=\"ontologies\",\n",
    "    location=\":memory:\",\n",
    "    # path=\"./data/qdrant\",\n",
    "    # Run Qdrant as a service for production use:\n",
    "    # url=\"http://localhost:6333\",\n",
    "    # prefer_grpc=True,\n",
    ")\n",
    "# vectorstore = FAISS.from_documents(documents=docs, embedding=flag_embeddings)\n",
    "# K is the number of source documents retrieved\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "llm = ChatOllama(model=\"llama3\")\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, max_tokens=None, timeout=None, max_retries=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 Initialize prompts and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the memory object that is used to add messages\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\"\n",
    ")\n",
    "# Add a \"memory\" key to the input object\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"),\n",
    ")\n",
    "\n",
    "# Prompt to reformulate the question using the chat history\n",
    "reform_template = \"\"\"Given the following chat history and a follow up question,\n",
    "rephrase the follow up question to be a standalone straightforward question, in its original language.\n",
    "Do not answer the question! Just rephrase reusing informations from the chat history.\n",
    "Make it one short sentence short and straight to the point.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "REFORM_QUESTION_PROMPT = ChatPromptTemplate.from_template(reform_template)\n",
    "\n",
    "# Prompt to ask to answer the reformulated question\n",
    "answer_template = \"\"\"Briefly answer the question reusing the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(answer_template)\n",
    "\n",
    "# Format how the ontology concepts are passed as context to the LLM\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n",
    "    template=\"User question: {page_content}\\n```sparql\\n# {endpoint_url}\\n{query}```\"\n",
    ")\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    # print(\"Formatted docs:\", doc_strings)\n",
    "    return document_separator.join(doc_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⛓️ Define the chain\n",
    "\n",
    "`itemgetter()` is used to retrieve objects defined in the previous step in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformulate the question using chat history\n",
    "reformulated_question = {\n",
    "    \"reformulated_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "    }\n",
    "    | REFORM_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "# Retrieve the documents using the reformulated question\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"reformulated_question\") | retriever,\n",
    "    \"question\": lambda x: print(\"💭 Reformulated question:\", x[\"reformulated_question\"]) or x[\"reformulated_question\"],\n",
    "    # \"question\": lambda x: x[\"reformulated_question\"],\n",
    "}\n",
    "# Construct the inputs for the final prompt using retrieved documents\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "# Generate the answer using the retrieved documents and answer prompt\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "# Put the chain together\n",
    "final_chain = loaded_memory | reformulated_question | retrieved_documents | answer\n",
    "\n",
    "def stream_chain(final_chain, memory: ConversationBufferMemory, inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    \"\"\"Ask question, stream the answer output, and return the answer with source documents.\"\"\"\n",
    "    output = {\"answer\": \"\"}\n",
    "    for chunk in final_chain.stream(inputs):\n",
    "        # print(chunk)\n",
    "        if \"docs\" in chunk:\n",
    "            output[\"docs\"] = [doc.dict() for doc in chunk[\"docs\"]]\n",
    "            print(\"📚 Documents retrieved:\")\n",
    "            for doc in output[\"docs\"]:\n",
    "                print(f\"· {doc['page_content']}\") # ({doc['metadata']['query']})\n",
    "            # print(json.dumps(output[\"docs\"], indent=2))\n",
    "        if \"answer\" in chunk:\n",
    "            output[\"answer\"] += chunk[\"answer\"].content\n",
    "            print(chunk[\"answer\"].content, end=\"\", flush=True)\n",
    "    # Add message to chat history\n",
    "    memory.save_context(inputs, {\"answer\": output[\"answer\"]})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗨️ Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💭 Reformulated question: What is the best way to obtain the Human Genome Organization (HGNC) symbol for a specific protein?\n",
      "📚 Documents retrieved:\n",
      "· Map UniProt to HGNC identifiers and Symbols\n",
      "· Find any uniprot entry, or an uniprot entries domain or component which has a name 'HLA class I histocompatibility antigen, B-73 alpha chain'\n",
      "· For the human entry P05067 (Amyloid-beta precursor protein) find the gene start ends in WikiData\n",
      "· Find any uniprot entry which has a name 'HLA class I histocompatibility antigen, B-73 alpha chain'\n",
      "· Construct new triples of the type 'HumanProtein' from all human UniProt entries\n",
      "Based on the provided context, you can map UniProt IDs to HGNC identifiers and symbols using the following SPARQL query:\n",
      "\n",
      "```sparql\n",
      "PREFIX up: <http://purl.uniprot.org/core/>\n",
      "PREFIX uniprotkb: <http://purl.uniprot.org/uniprot/>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "SELECT ?uniprot ?hgnc ?hgncSymbol\n",
      "WHERE {\n",
      "  VALUES (?acc) {('P05067') ('P00750')}\n",
      "  BIND(iri(concat(str(uniprotkb:), ?acc)) AS ?uniprot)\n",
      "  ?uniprot rdfs:seeAlso ?hgnc .\n",
      "  ?hgnc up:database <http://purl.uniprot.org/database/HGNC> ;\n",
      "       rdfs:comment ?hgncSymbol .\n",
      "}\n",
      "```\n",
      "\n",
      "This query uses the `VALUES` clause to specify a list of UniProt IDs. It then binds each ID to a URI using the `BIND` function, and queries for the HGNC identifier (`?hgnc`) that is associated with each UniProt entry via the `rdfs:seeAlso` property. Finally, it retrieves the HGNC symbol (`?hgncSymbol`) from the HGNC database.\n",
      "\n",
      "To obtain the HGNC symbol for a specific protein, you can simply replace `'P05067'` and `'P00750'` in the above query with the UniProt ID of the protein you're interested in."
     ]
    }
   ],
   "source": [
    "# set_debug(True)   # Uncomment to enable detailed LangChain debugging\n",
    "output = stream_chain(final_chain, memory, {\n",
    "    \"question\": \"How can I retrieve the HGNC symbol for a protein?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💭 Reformulated question: What is the SPARQL query to retrieve the HGNC symbol for the protein with UniProt ID P68871?\n",
      "📚 Documents retrieved:\n",
      "· Map UniProt to HGNC identifiers and Symbols\n",
      "· Find the orthologous proteins for UniProtKB entry P05067 using the <a href=\"http://www.orthod.org\">OrthoDB database</a>\n",
      "· Select all human UniProt entries with a sequence variant that leads to a tyrosine to phenylalanine substitution\n",
      "· Find any uniprot entry, or an uniprot entries domain or component which has a name 'HLA class I histocompatibility antigen, B-73 alpha chain'\n",
      "· Find the similar proteins for UniProtKB entry P05067 sorted by UniRef cluster identity\n",
      "You can use the first SPARQL query provided:\n",
      "\n",
      "```sparql\n",
      "# https://sparql.uniprot.org/sparql/\n",
      "PREFIX up: <http://purl.uniprot.org/core/>\n",
      "PREFIX uniprotkb: <http://purl.uniprot.org/uniprot/>\n",
      "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "SELECT\n",
      "  ?uniprot\n",
      "  ?hgnc\n",
      "  ?hgncSymbol\n",
      "WHERE\n",
      "{\n",
      "  # A space separated list of UniProt primary accessions.\n",
      "  VALUES (?acc) {('P68871')}\n",
      "  BIND(iri(concat(str(uniprotkb:), ?acc)) AS ?uniprot)\n",
      "  ?uniprot rdfs:seeAlso ?hgnc .\n",
      "  ?hgnc up:database <http://purl.uniprot.org/database/HGNC> ;\n",
      "       rdfs:comment ?hgncSymbol .\n",
      "}\n",
      "```\n",
      "\n",
      "Replace `'P68871'` with the desired UniProt ID."
     ]
    }
   ],
   "source": [
    "# set_debug(True)   # Uncomment to enable detailed LangChain debugging\n",
    "output = stream_chain(final_chain, memory, {\n",
    "    \"question\": \"Could you write the complete query for protein P68871?\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libre-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
